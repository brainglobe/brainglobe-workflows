name: tests

on:
  push:
    branches:
      - "main"
    tags:
      - "*"
  pull_request:
  schedule:
    # Runs at 6:10am UTC on Monday
    - cron: "10 6 * * 1"
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  linting:
    name: Linting
    runs-on: ubuntu-latest
    steps:
      - uses: neuroinformatics-unit/actions/lint@v2

  manifest:
    name: Check manifest
    runs-on: ubuntu-latest
    steps:
      - uses: neuroinformatics-unit/actions/check_manifest@v2

  test:
    needs: [linting, manifest]
    name: ${{ matrix.os }} py${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    env:
      KERAS_BACKEND: torch
    strategy:
      fail-fast: false
      matrix:
        # Run tests on ubuntu across all supported versions
        python-version: ["3.11", "3.12", "3.13"]
        os: [ubuntu-latest]
        # Include a Windows test and new Mac runs
        include:
          - os: macos-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"
    steps:
      # Optimize Windows temporary file storage by moving to the faster D: drive
      - name: Optimize Windows Temporary Storage
        if: matrix.os == 'windows-latest'
        run: |
          mkdir "D:\\Temp"
          echo "TEMP=D:\\Temp" >> $env:GITHUB_ENV
          echo "TMP=D:\\Temp" >> $env:GITHUB_ENV

      # Cache brainglobe directory to avoid re-downloading atlases
      - name: Cache brainglobe directory
        uses: actions/cache@v3
        with:
          path:
            | # Ensure we don't cache any interrupted atlas download and extraction, for e.g. if we cancel the workflow manually
            ~/.brainglobe
            !~/.brainglobe/atlas.tar.gz
          key: atlases-models
          fail-on-cache-miss: true
          enableCrossOsArchive: true

      # Install additional dependencies on macOS
      - name: Install HDF5 libraries (needed on M1 Macs only)
        if: matrix.os == 'macos-latest'
        run: |
          brew install hdf5

      # Cache test data to speed up workflow tests
      - name: Cache data for cellfinder workflow tests
        uses: actions/cache@v3
        with:
          path: "~/.brainglobe-tests"
          key: cellfinder-test-data
          fail-on-cache-miss: true
          enableCrossOsArchive: true

      # Run test suite across different environments
      - uses: neuroinformatics-unit/actions/test@v2
        with:
          python-version: ${{ matrix.python-version }}
          secret-codecov-token: ${{ secrets.CODECOV_TOKEN }}

      - name: Notify slack on scheduled failure
        if: failure() && github.event_name == 'schedule'
        uses: ravsamhq/notify-slack-action@v2
        with:
          status: ${{ job.status }} # Required
          notify_when: "failure"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_NOTIFYBOT_WEBHOOK_URL }} # required

  benchmarks:
    name: Check benchmarks
    runs-on: ubuntu-latest
    # Set shell in login mode as global setting for the job
    defaults:
      run:
        shell: bash -l {0}
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - name: Checkout brainglobe-workflows repository
        uses: actions/checkout@v4

      - name: Create and activate conda environment # we need conda for asv management of environments
        uses: conda-incubator/setup-miniconda@v2.1.1
        with:
          miniconda-version: py310_24.1.2-0 # we need conda<24.3, see https://github.com/airspeed-velocity/asv/pull/1397
          python-version: ${{ matrix.python-version }}
          activate-environment: asv-only

      - name: Install asv (Air Speed Velocity) for benchmarking
        run: |
          pip install --upgrade pip
          pip install asv

      - name: Run asv check with pip dependencies
        working-directory: ${{ github.workspace }}/benchmarks
        run: |
          asv check -v --config $GITHUB_WORKSPACE/benchmarks/asv.pip.conf.json

      - name: Run asv check with latest-github dependencies
        working-directory: ${{ github.workspace }}/benchmarks
        run: |
          asv check -v --config $GITHUB_WORKSPACE/benchmarks/asv.latest-github.conf.json

  build_sdist_wheels:
    name: Build source distribution
    needs: [test]
    if: github.event_name == 'push' && github.ref_type == 'tag'
    runs-on: ubuntu-latest
    steps:
      - uses: neuroinformatics-unit/actions/build_sdist_wheels@v2

  upload_all:
    name: Publish build distributions
    needs: [build_sdist_wheels]
    if: github.event_name == 'push' && github.ref_type == 'tag'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: artifact
          path: dist
      - uses: pypa/gh-action-pypi-publish@release/v1
        with:
          user: __token__
          password: ${{ secrets.TWINE_API_KEY }}
